{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f75b20",
   "metadata": {},
   "source": [
    "# 1. Basic API Call, (Use ollama.AsyncClient)\n",
    "- https://www.youtube.com/watch?v=bZzyPscbtI8&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import asyncio\n",
    "\n",
    "# #Only for Jupyter Notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main():     \n",
    "    OLLAMA_MODEL = 'llama3.1:8b'  \n",
    "    \n",
    "    ollama_async_client = ollama.AsyncClient()     \n",
    "    await ollama_async_client.pull(OLLAMA_MODEL)   \n",
    "     \n",
    "    response = await ollama_async_client.chat(        \n",
    "            model=OLLAMA_MODEL, \n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Write a limerick about the Python programming language.\"\n",
    "    result = asyncio.run(main())\n",
    "    print(f\"\\n Limerick Output:\\n\\n {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9402dfe0",
   "metadata": {},
   "source": [
    "# 2. Basic API Call, (Use OpenAI and wrap by instructor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7accf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Limerick Output:\n",
      "\n",
      " Here is a limerick about Python:\n",
      "\n",
      "There once was a coder so fine,\n",
      "Whose skills in Python did shine.\n",
      "She wrote with great care,\n",
      "And her code was fair,\n",
      "Now her projects are truly divine.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "#Only for Jupyter Notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "            \n",
    "async def main():     \n",
    "    # Connect OpenAI client to local Ollama server\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1/\",\n",
    "        api_key=\"not-needed\"   # Ollama ignores it\n",
    "    )\n",
    "\n",
    "    # Add instructor wrapper for Pydantic validation\n",
    "    client = instructor.patch(client)    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.1:8b\",\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.3                 \n",
    "    )\n",
    "    \n",
    "    # The `create` method returns a `ChatCompletion` object, not a dictionary.\n",
    "    # We must access its attributes using dot notation, not subscription (`[]`).\n",
    "    # The response text is located within the `choices` list, then the `message` object.\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':   \n",
    "    prompt = \"Write a limerick about the Python programming language.\"\n",
    "    result = asyncio.run(main())\n",
    "    print(f\"\\n Limerick Output:\\n\\n {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-312 (.venv-312)",
   "language": "python",
   "name": ".venv-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
